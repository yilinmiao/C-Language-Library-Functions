{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkQNFAf1dMdcku1FoeByUT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yilinmiao/C-Language-Library-Functions/blob/master/encoding_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RuzMNUu4vN-D"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import string\n",
        "import re\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization Steps\n",
        "\n",
        "In this exercise, you'll code your own tokenizer from scratching using base\n",
        "Python!\n",
        "\n",
        "You might normally start with a pretrained tokenizer, but this exercise will\n",
        "help you get to know see some of the tokenization steps better."
      ],
      "metadata": {
        "id": "DncWiPbBDuqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = '''Mr. Louis continued to say, \"Penguins are important,\n",
        "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
        "'''\n",
        "\n",
        "print(sample_text)"
      ],
      "metadata": {
        "id": "7_v0fnNw1lFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3ba4c1-267e-4e1a-ac60-ad2c37a2f886"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mr. Louis continued to say, \"Penguins are important, \n",
            "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Sample Text\n",
        "\n",
        "Let's first define some sample text you will use to test your tokenization\n",
        "steps."
      ],
      "metadata": {
        "id": "nSZsKJjmDx9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization\n",
        "\n",
        "This step is where you'll normalize your text by converting to lowercase,\n",
        "removing accented characters, etc.\n",
        "\n",
        "For example, the text:\n",
        "```\n",
        "Did Uncle Max like the jalapeño dip?\n",
        "```\n",
        "might be normalized to:\n",
        "```\n",
        "did uncle max like the jalapeno dip\n",
        "```"
      ],
      "metadata": {
        "id": "qupO41xL5h25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text: str) -> str:\n",
        "    # COMPLETE: Normalize incoming text; can be multiple actions\n",
        "    # Only keep ASCII letters, numbers, punctuation, and whitespace characters\n",
        "    acceptable_characters = (\n",
        "        string.ascii_letters\n",
        "        + string.digits\n",
        "        + string.punctuation\n",
        "        + string.whitespace  # jalapeño -> jalapeo\n",
        "    )\n",
        "    normalized_text = ''.join(\n",
        "        filter(lambda letter: letter in acceptable_characters, text)\n",
        "    )\n",
        "    # Make text lower-case\n",
        "    normalized_text = normalized_text.lower()\n",
        "    return normalized_text"
      ],
      "metadata": {
        "id": "mdvxwfTo4wUK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out your normalization\n",
        "normalize_text(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TP8lqGGu5jG-",
        "outputId": "1025068d-78e7-4b5d-bf77-8e68a28e5ccd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mr. louis continued to say, \"penguins are important, \\nbut we mustn\\'t forget the nuumber 1 priority: the reader!\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretokenization\n",
        "\n",
        "This step will take in the normalized text and pretokenize the text into a list\n",
        "of smaller pieces.\n",
        "\n",
        "For example, the text:\n",
        "```\n",
        "Did Uncle Max like the jalapeño dip?\n",
        "```\n",
        "might be normalized & then pretokenized to:\n",
        "```\n",
        "[\n",
        "    'did',\n",
        "    'uncle',\n",
        "    'max',\n",
        "    'like',\n",
        "    'the',\n",
        "    'jalapeno',\n",
        "    'dip?',\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "3_noarygB2MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretokenize_text(text: str) -> list[str]:\n",
        "    # COMPLETE: Pretokenize normalized text\n",
        "    # Split based on spaces\n",
        "    smaller_pieces = text.split()\n",
        "    return smaller_pieces"
      ],
      "metadata": {
        "id": "clp87xKY5cJ_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out your pretokenization step (after normalizing the text)\n",
        "normalized_text = normalize_text(sample_text)\n",
        "pretokenize_text(normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85FTM3qbB_RJ",
        "outputId": "fac2353f-549f-4128-8324-6f21cca1ef03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mr.',\n",
              " 'louis',\n",
              " 'continued',\n",
              " 'to',\n",
              " 'say,',\n",
              " '\"penguins',\n",
              " 'are',\n",
              " 'important,',\n",
              " 'but',\n",
              " 'we',\n",
              " \"mustn't\",\n",
              " 'forget',\n",
              " 'the',\n",
              " 'nuumber',\n",
              " '1',\n",
              " 'priority:',\n",
              " 'the',\n",
              " 'reader!\"']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "This step will take in the list of pretokenized pieces (after the text has\n",
        "been normalized) into the tokens that will be used.\n",
        "\n",
        "For example, the text:\n",
        "```\n",
        "Did Uncle Max like the jalapeño dip?\n",
        "```\n",
        "might be normalized, pretokenized, and then tokenized to:\n",
        "```\n",
        "[\n",
        "    'did',\n",
        "    'uncle',\n",
        "    'max',\n",
        "    'like',\n",
        "    'the',\n",
        "    'jalapeno',\n",
        "    'dip'\n",
        "    '?',\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "QXRj8NwuCGuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine normalization and pretokenization steps before breaking things further\n",
        "def tokenize_text(text: str) -> list[str]:\n",
        "    # Apply created steps\n",
        "    normalized_text: str = normalize_text(text)\n",
        "    pretokenized_text: list[str] = pretokenize_text(normalized_text)\n",
        "    # COMPLETE: Go through pretokenized text to create a list of tokens\n",
        "    tokens = []\n",
        "    # Small 'pieces' to make full tokens\n",
        "    for word in pretokenized_text:\n",
        "        tokens.extend(\n",
        "            re.findall(\n",
        "                f'[\\w]+|[{string.punctuation}]', # Split word at punctuations\n",
        "                word,\n",
        "            )\n",
        "        )\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "jx5UkMj5CFIL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out your tokenization (that uses normalizing & pretokenizing functions)\n",
        "tokenize_text(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzbogYKoCOKz",
        "outputId": "7aea86fd-2b76-4074-d0ac-0767b8f9ce67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mr',\n",
              " '.',\n",
              " 'louis',\n",
              " 'continued',\n",
              " 'to',\n",
              " 'say',\n",
              " ',',\n",
              " '\"',\n",
              " 'penguins',\n",
              " 'are',\n",
              " 'important',\n",
              " ',',\n",
              " 'but',\n",
              " 'we',\n",
              " 'mustn',\n",
              " \"'\",\n",
              " 't',\n",
              " 'forget',\n",
              " 'the',\n",
              " 'nuumber',\n",
              " '1',\n",
              " 'priority',\n",
              " ':',\n",
              " 'the',\n",
              " 'reader',\n",
              " '!',\n",
              " '\"']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocessing\n",
        "\n",
        "This final step will take in the list of tokens from the original text and add\n",
        "any special tokens to the text.\n",
        "\n",
        "For example, the text:\n",
        "```\n",
        "Did Uncle Max like the jalapeño dip?\n",
        "```\n",
        "might be normalized, pretokenized, and then tokenized to:\n",
        "```\n",
        "[\n",
        "    '[BOS]',\n",
        "    'did',\n",
        "    'uncle',\n",
        "    'max',\n",
        "    'like',\n",
        "    'the',\n",
        "    'jalapeno',\n",
        "    'dip'\n",
        "    '?',\n",
        "    '[EOS]',\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "mIJdSVNBDAkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful for some tasks\n",
        "def postprocess_tokens(tokens: list[str]) -> list[str]:\n",
        "    # COMPLETE: Add beginning and end of sequence tokens to your tokenized text\n",
        "    # Can use a format like '[BOS]' & '[EOS]'\n",
        "    bos_token = '[BOS]'\n",
        "    eos_token = '[EOS]'\n",
        "    updated_tokens = (\n",
        "        [bos_token]\n",
        "        + tokens\n",
        "        + [eos_token]\n",
        "    )\n",
        "    return updated_tokens"
      ],
      "metadata": {
        "id": "ZyDpN3dlCR4C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test full pipeline (normalizing, pretokenizing, tokenizing, & postprocessing)\n",
        "tokens = tokenize_text(sample_text)\n",
        "tokens = postprocess_tokens(tokens)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z5nKWgoDLoz",
        "outputId": "e226792d-0c87-4fa3-f02e-61ad5442c209"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding & Decoding"
      ],
      "metadata": {
        "id": "YoIYPiOsDWt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding Text to Token IDs\n",
        "\n",
        "Create an encoder (`encode()`) that will encode the token strings to integer IDs\n",
        "by defining how to map each token to a unique ID.\n",
        "\n",
        "> HINT:\n",
        ">\n",
        "> An easy method is to assign an arbitrary integer to each unique token from\n",
        "> the corpus by iterating through the unique tokens."
      ],
      "metadata": {
        "id": "Py4bfyADDZnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample corpus (normally this would be much bigger)\n",
        "sample_corpus = (\n",
        "    '''Mr. Louis continued to say, \"Penguins are important, \\nbut we mustn't forget the nuumber 1 priority: the READER!\"''',\n",
        "    '''BRUTUS:\\nHe's a lamb indeed, that baes like a bear.''',\n",
        "    '''Both by myself and many other friends:\\mBut he, his own affections' counsellor,\\nIs to himself--I will not say how true--\\nBut to himself so secret and so close,'''\n",
        ")"
      ],
      "metadata": {
        "id": "Hw3VE_DhDY8R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE: Create an encoder to transform token strings to IDs using the sample\n",
        "# corpus as the basis of your encoding\n",
        "\n",
        "# COMPLETE: Your code here (might be outside of the encode() function scope)\n",
        "\n",
        "# Retrieve unique tokens (from the pipeline defined above) in a set\n",
        "unique_tokens = set()\n",
        "for text in sample_corpus:\n",
        "    tokens_from_text = tokenize_text(text)\n",
        "    tokens_from_text = postprocess_tokens(tokens_from_text)\n",
        "    unique_tokens.update(tokens_from_text)\n",
        "\n",
        "# Create mapping (dictionary) for unique tokens using arbitrary & unique IDs\n",
        "token2id = defaultdict(lambda : 0) # Allow for unknown tokens to map to 0\n",
        "token2id |= {\n",
        "    token: idx\n",
        "    for idx, token in enumerate(unique_tokens, 1) # Skip 0 (represents unknown)\n",
        "}\n",
        "\n",
        "# A mapping for IDs to convert back to token\n",
        "id2token = defaultdict(lambda : '[UNK]') # Allow for unknown token ('[UNK]')\n",
        "id2token |= {\n",
        "    idx: token\n",
        "    for token, idx in token2id.items()\n",
        "}\n",
        "\n",
        "\n",
        "def encode(tokens: list[str]) -> list[int]:\n",
        "    # COMPLETE: Complete this function to encode tokens to integer IDs\n",
        "    encoded_tokens = [token2id[token] for token in tokens]\n",
        "    return encoded_tokens\n"
      ],
      "metadata": {
        "id": "YR9XdKYpDdgi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test `encode()`"
      ],
      "metadata": {
        "id": "9F2yj3NqECc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use sample text for testing\n",
        "sample_text = sample_corpus[0]\n",
        "# Create tokens (to be fed to encode())\n",
        "tokens = tokenize_text(sample_text)\n",
        "tokens = postprocess_tokens(tokens)\n",
        "print(f'Tokens:\\n{tokens}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oVoAKLGDdix",
        "outputId": "852c6a43-c908-4344-f97a-285cfab0213e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test encode()\n",
        "encoded_tokens = encode(tokens)\n",
        "print(f'Encoded Tokens:\\n{encoded_tokens}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma5_wszeDY_Z",
        "outputId": "b11c0556-b25d-4509-cd2c-7738ec552515"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Tokens:\n",
            "[6, 3, 42, 35, 18, 9, 2, 37, 51, 38, 7, 1, 37, 28, 39, 40, 47, 14, 56, 23, 13, 41, 34, 31, 23, 52, 24, 51, 4]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding Token IDs to Text\n",
        "\n",
        "Based on your enocder you created (`encode()`), create a decoder (`decode()`) to\n",
        "take a list of token IDs and map them to their associated token."
      ],
      "metadata": {
        "id": "U2Hp2FpkFU7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE: Create an encoder to transform IDs (from encode()) to token strings\n",
        "\n",
        "def decode(ids: list[int]) -> list[str]:\n",
        "    # COMPLETE: Complete this function to decode integer IDs to token strings\n",
        "    token_strings = [id2token[idx] for idx in ids]\n",
        "    return token_strings"
      ],
      "metadata": {
        "id": "qh9DZXWDFVuD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test `decode()`"
      ],
      "metadata": {
        "id": "2PX0bbkYFbI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use sample text for testing\n",
        "sample_text = sample_corpus[0]\n",
        "# Create tokens\n",
        "tokens = tokenize_text(sample_text)\n",
        "tokens = postprocess_tokens(tokens)\n",
        "print(f'Tokens:\\n{tokens}\\n')\n",
        "\n",
        "# Create token IDs (to be fed to decode())\n",
        "encoded_tokens = encode(tokens)\n",
        "print(f'Encoded Tokens:\\n{encoded_tokens}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrdxvkXcFYG-",
        "outputId": "e9a4780a-147a-40d1-f03f-08f21fe725ec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n",
            "\n",
            "Encoded Tokens:\n",
            "[6, 3, 42, 35, 18, 9, 2, 37, 51, 38, 7, 1, 37, 28, 39, 40, 47, 14, 56, 23, 13, 41, 34, 31, 23, 52, 24, 51, 4]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out decode()\n",
        "decoded_tokens = decode(encoded_tokens)\n",
        "print(f'Decoded Tokens:\\n{decoded_tokens}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCy8XFYLFetO",
        "outputId": "865ff45f-66f5-40ac-cbda-01b717b4d801"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded Tokens:\n",
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}